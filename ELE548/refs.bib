@article{Wu2011.2,
   abstract = {The shared last-level caches in CMPs play an important role in improving application performance and reducing off-chip memory bandwidth requirements. In order to use LLCs more efficiently, recent research has shown that changing the re-reference prediction on cache insertions and cache hits can significantly improve cache performance. A fundamental challenge, however, is how to best predict the re-reference pattern of an incoming cache line. This paper shows that cache performance can be improved by correlating the re-reference behavior of a cache line with a unique signature. We investigate the use of memory region, program counter, and instruction sequence history based signatures. We also propose a novel Signature-based Hit Predictor (SHiP) to learn the re-reference behavior of cache lines belonging to each signature. Overall, we find that SHiP offers substantial improvements over the baseline LRU replacement and state-of-the-art replacement policy proposals. On average, SHiP improves sequential and multipro-grammed application performance by roughly 10% and 12% over LRU replacement, respectively. Compared to recent replacement policy proposals such as Seg-LRU and SDBP, SHiP nearly doubles the performance gains while requiring less hardware overhead.},
   author = {Carole-Jean Wu and Aamer Jaleel and Will Hasenplaugh and Margaret Martonosi and Simon C Steely and Joel Emer},
   isbn = {9781450310536},
   keywords = {B83 [Hardware]: Memory Structures General Terms Design,Performance Keywords Replacement,Reuse Distance Prediction,Shared Cache},
   title = {SHiP: Signature-based Hit Predictor for High Performance Caching},
   year = {2011},
}
@article{Gao2010,
   abstract = {In this paper we present a high performance cache replacement algorithm called Dueling Segmented LRU replacement algorithm with adaptive Bypassing (DSB). The base algorithm is Segmented LRU (SLRU) replacement algorithm originally proposed for disk cache management. We introduce three enhancements to the base SLRU algorithm. First, a newly allocated line could be randomly promoted for better protection. Second, an aging algorithm is used to remove stale cache lines. Most importantly, we propose a novel scheme to track whether cache bypassing is effective. Based on the tracking results, we can adaptively adjust bypassing to fit workload behavior. DSB algorithm is implemented with a policy selector to dynamically select two variants of SLRU algorithms with different enhancements.},
   author = {Hongliang Gao and Chris Wilkerson},
   title = {A Dueling Segmented LRU Replacement Algorithm with Adaptive Bypassing},
}
@book{Seznec2010,
   abstract = {Title from The ACM Digital Library.},
   author = {André. Seznec and ACM Digital Library. and Sigarch.},
   isbn = {9781450300537},
   pages = {510},
   publisher = {ACM},
   title = {Proceedings of the 37th annual international symposium on Computer architecture.},
   year = {2010},
}
@article{Khan2010,
   abstract = {Caches mitigate the long memory latency that limits the performance of modern processors. However, caches can be quite inefficient. On average, a cache block in a 2MB L2 cache is dead 59% of the time, i.e., it will not be referenced again before it is evicted. Increasing cache efficiency can improve performance by reducing miss rate, or alternately, improve power and energy by allowing a smaller cache with the same miss rate. This paper proposes using predicted dead blocks to hold blocks evicted from other sets. When these evicted blocks are referenced again, the access can be satisfied from the other set, avoiding a costly access to main memory. The pool of predicted dead blocks can be thought of as a virtual victim cache. For a set of memory-intensive single-threaded workloads, a virtual victim cache in a 16-way set associative 2MB L2 cache reduces misses by 26%, yields an geometric mean speedup of 12.1% and improves cache efficiency by 27% on average, where cache efficiency is defined as the average time during which cache blocks contain live information. This virtual victim cache yields a lower average miss rate than a fully-associative LRU cache of the same capacity. For a set of multi-core workloads, the virtual victim cache improves throughput performance by 4% over LRU while improving cache efficiency by 62%. Alternately, a 1.7MB virtual victim cache achieves about the same performance as a larger 2MB L2 cache, reducing the number of SRAM cells required by 16%, thus maintaining performance while reducing power and area.},
   author = {Samira Khan and Daniel A Jiménez and Doug Burger and Babak Falsafi},
   isbn = {9781450301787},
   issue = {10},
   keywords = {B32 [Hardware]: Memory Structures-Cache Memories General Terms Design,Performance Keywords microarchitecture,cache management,prediction},
   title = {Using Dead Blocks as a Virtual Victim Cache},
}
@article{Daniel2010,
   abstract = {We present a cache replacement and bypass policy driven by dead block prediction. A block is considered dead is it will be replaced before it will be used again. If dead blocks can be identified, then they can be replaced early. If a block is predicted to be "dead on arrival," i.e., it will not be accessed again after it is placed in the cache, then it can bypass the cache. The predictor is based on one simple observation: if a block becomes dead after being touched by a particular instruction, then other blocks touched by that instruction are also likely to become dead. Ideally, we would track the tendency of blocks to become dead for every instruction accessing the cache. However, to fit within a realistic hardware budget, we sample only a few sets from the cache. This paper describes our sampling dead block predictor and the techniques used to make it fit within the allow hardware budget for the cache replacement contest.},
   author = {Daniel A Jiménez},
   title = {Dead Block Replacement and Bypass with a Sampling Predictor},
}
@article{Kharbutli2008,
   abstract = {Recent studies have shown that, in highly associative caches, the performance gap between the Least Recently Used (LRU) and the theoretical optimal replacement algorithms is large, motivating the design of alternative replacement algorithms to improve cache performance. In LRU replacement, a line, after its last use, remains in the cache for a long time until it becomes the LRU line. Such dead lines unnecessarily reduce the cache capacity available for other lines. In addition, in multilevel caches, temporal reuse patterns are often inverted, showing in the L1 cache but, due to the filtering effect of the L1 cache, not showing in the L2 cache. At the L2, these lines appear to be brought in the cache but are never reaccessed until they are replaced. These lines unnecessarily pollute the L2 cache. This paper proposes a new counter-based approach to deal with the above problems. For the former problem, we predict lines that have become dead and replace them early from the L2 cache. For the latter problem, we identify never-reaccessed lines, bypass the L2 cache, and place them directly in the L1 cache. Both techniques are achieved through a single counter-based mechanism. In our approach, each line in the L2 cache is augmented with an event counter that is incremented when an event of interest such as certain cache accesses occurs. When the counter reaches a threshold, the line 'expires' and becomes replaceable. Each line's threshold is unique and is dynamically learned. We propose and evaluate two new replacement algorithms: Access Interval Predictor (AIP) and Live-time Predictor (LvP). AIP and LvP speed up 10 capacity-constrained SPEC2000 benchmarks by up to 48 percent and 15 percent on average (7 percent on average for the whole 21 Spec2000 benchmarks). Cache bypassing further reduces L2 cache pollution and improves the average speedups to 17 percent (8 percent for the whole 21 Spec2000 benchmarks). © 2008 IEEE.},
   author = {Mazen Kharbutli and Yan Solihin},
   doi = {10.1109/TC.2007.70816},
   issn = {00189340},
   issue = {4},
   journal = {IEEE Transactions on Computers},
   keywords = {Cache bypassing,Cache misses,Cache replacement algorithms,Caches,Counter-based algorithms},
   month = {4},
   pages = {433-447},
   title = {Counter-based cache replacement and bypassing algorithms},
   volume = {57},
   year = {2008},
}
@article{,
   abstract = {Data caches in general-purpose microprocessors often contain mostly dead blocks and are thus used inefficiently. To improve cache efficiency, dead blocks should be identified and evicted early. Prior schemes predict the death of a block immediately after it is accessed; however, these schemes yield lower prediction accuracy and coverage. Instead, we find that predicting the death of a block when it just moves out of the MRU position gives the best tradeoff between timeliness and prediction accuracy/coverage. Furthermore, the individual reference history of a block in the L1 cache can be irregular because of data/control dependence. This paper proposes a new class of dead-block predictors that predict dead blocks based on bursts of accesses to a cache block. A cache burst begins when a block becomes MRU and ends when it becomes non-MRU. Cache bursts are more predictable than individual references because they hide the irregularity of individual references. When used at the L1 cache, the best burst-based predictor can identify 96% of the dead blocks with a 96% accuracy. With the improved dead-block predictors, we evaluate three ways to increase cache efficiency by eliminating dead blocks early: replacement optimization, bypassing, and prefetching. The most effective approach, prefetching into dead blocks, increases the average L1 efficiency from 8% to 17% and the L2 efficiency from 17% to 27%. This increased cache efficiency translates into higher overall performance: prefetching into dead blocks outperforms the same prefetch scheme without dead-block prediction by 12% at the L1 and by 13% at the L2.},
   author = {Haiming Liu and Michael Ferdman and Jaehyuk Huh and Doug Burger},
   title = {Cache Bursts: A New Approach for Eliminating Dead Blocks and Increasing Cache Efficiency},
}
@article{Qureshi2007,
   abstract = {The commonly used LRU replacement policy is susceptible to thrashing for memory-intensive workloads that have a working set greater than the available cache size. For such applications, the majority of lines traverse from the MRU position to the LRU position without receiving any cache hits, resulting in inefficient use of cache space. Cache performance can be improved if some fraction of the working set is retained in the cache so that at least that fraction of the working set can contribute to cache hits. We show that simple changes to the insertion policy can significantly reduce cache misses for memory-intensive workloads. We propose the LRU Insertion Policy (LIP) which places the incoming line in the LRU position instead of the MRU position. LIP protects the cache from thrashing and results in close to optimal hit-rate for applications that have a cyclic reference pattern. We also propose the Bimodal Insertion Policy (BIP) as an enhancement of LIP that adapts to changes in the working set while maintaining the thrashing protection of LIP. We finally propose a Dynamic Insertion Policy (DIP) to choose between BIP and the traditional LRU policy depending on which policy incurs fewer misses. The proposed insertion policies do not require any change to the existing cache structure, are trivial to implement, and have a storage requirement of less than two bytes. We show that DIP reduces the average MPKI of the baseline 1MB 16-way L2 cache by 21%, bridging two-thirds of the gap between LRU and OPT.},
   author = {Moinuddin K Qureshi and Aamer Jaleel and Yale N Patt and Simon C Steely Jr and Joel Emer},
   isbn = {9781595937063},
   keywords = {B32 [Design Styles]: Cache memories General Terms: Design,Performance Keywords: Replacement,Set Dueling,Set Sampling,Thrashing},
   title = {Adaptive Insertion Policies for High Performance Caching},
   year = {2007},
}
@article{santh2007,
   abstract = {High performance processors employ hardware data prefetching to reduce the negative performance impact of large main memory latencies. While prefetching improves performance substantially on many programs, it can significantly reduce performance on others. Also, prefetching can significantly increase memory bandwidth requirements. This paper proposes a mechanism that incorporates dynamic feedback into the design of the prefetcher to increase the performance improvement provided by prefetching as well as to reduce the negative performance and bandwidth impact of prefetching. Our mechanism estimates prefetcher accuracy, prefetcher timeliness, and prefetcher-caused cache pollution to adjust the aggressiveness of the data prefetcher dynamically. We introduce a new method to track cache pollution caused by the prefetcher at run-time. We also introduce a mechanism that dynamically decides where in the LRU stack to insert the prefetched blocks in the cache based on the cache pollution caused by the prefetcher. Using the proposed dynamic mechanism improves average performance by 6.5% on 17 memory-intensive benchmarks in the SPEC CPU2000 suite compared to the best-performing conventional stream-based data prefetcher configuration, while it consumes 18.7% less memory bandwidth. Compared to a conventional stream-based data prefetcher configuration that consumes similar amount of memory bandwidth, feedback directed prefetching provides 13.6% higher performance. Our results show that feedback-directed prefetching eliminates the large negative performance impact incurred on some benchmarks due to prefetching, and it is applicable to stream-based prefetchers, global-history-buffer based delta correlation prefetchers, and PC-based stride prefetchers.},
   author = {Santhosh Srinath and Onur Mutlu and Hyesoon Kim and Yale N Patt},
   title = {Feedback Directed Prefetching: Improving the Performance and Bandwidth-Efficiency of Hardware Prefetchers},
}
@article{C2010,
   abstract = {In chip multiprocessors (CMPs), several high-performance cores typically compete for capacity in a shared last-level cache. This causes degraded and unpredictable memory performance for multiprogrammed and parallel workloads. In response, recent schemes apportion cache bandwidth and capacity in ways that offer better aggregate performance for the workloads. These schemes, however, focus primarily on relatively coarse-grained capacity management without concern for operating system process priority levels. In this work, we explore capacity management approaches that are both temporally and spatially more fine-grained than prior work. We also consider operating system priority levels as part of capacity management. We propose a capacity management mechanism based on timekeeping techniques that track the time interval since the last access to cached data. This Adaptive Timekeeping Replacement (ATR) scheme maintains aggregate cache occupancies that reflect the priority and footprint of each application. The key novelties of our work are (1) ATR offers a complete cache capacity management framework taking into account application priorities and memory characteristics, and (2) ATR's fine-grained cache capacity control is demonstrated to be effective and important in improving the performance of parallel workloads in addition to sequential ones. We evaluate our ideas using a full-system simulator and multiprogrammed workloads of both sequential and parallel applications. This is the first detailed study of shared cache capacity management considering thread behaviors in parallel applications. ATR outperforms an unmanaged system by as much as 1.63X and by an average of 1.19X. ATR's fine-grained temporal control is particularly important for parallel applications , which are expected to be increasingly prevalent in years to come.},
   author = {C Acm Reference Format: Wu and M Martonosi},
   doi = {10.1145/0000000.0000000},
   issue = {11},
   journal = {ACM Trans. Architec. Code Optim},
   keywords = {B83 [Hardware]: Memory Structures-General General Terms: Design,Performance Additional Key Words and Phrases: Cache decay,capacity management,shared resource management},
   pages = {27},
   title = {Adaptive Timekeeping Replacement: Fine-Grained Capacity Management for Shared CMP Caches},
   volume = {11},
   url = {http://doi.acm.org/10.1145/0000000.0000000},
   year = {2010},
}
@book{Sigarch2009,
   abstract = {"ACM Order Number 415094."},
   author = {Sigarch. and IEEE Computer Society. Technical Committee on Computer Architecture.},
   isbn = {9781605585260},
   pages = {497},
   publisher = {Association for Computing Machinery},
   title = {The 36th Annual International Symposium on Computer Architecture : Austin, Texas, USA, 20-24 June 2009 : Conference proceedings},
   year = {2009},
}
@report{Young2017.1,
   abstract = {In this paper, we analyze the state-of-the-art replacement policy based on Signature-Based Hit Prediction (SHiP). While we observe that the proposed SHiP implementation improves performance over the LRU policy on all configurations, we identify three areas of improvement. First, the policies for updating the counters in the Signature History Counter Table (SHCT) can be modified. Second, a broader range of insertion points for an incoming line can be used based on the value of the SHCT counter. Third, cache performance under prefetching can be enhanced by using a separate SHCT table for demand and prefetch lines and modifying the inser-tion/promotion of prefetch lines. We call our proposed implementation of SHiP with these enhancements as SHiP++. We evaluate SHiP++ on 17 benchmarks from the SPEC2006 suite. For the single-core without prefetching configuration, SHiP++ improves performance by 6.2% over LRU and 2.2% over SHiP. For the single-core with prefetching configuration, SHiP++ improves performance by 4.6% over LRU and 2.2% over SHiP. SHiP++ incurs an overhead of less than 20KB for managing the 2MB cache, well within the provisioned 32KB.},
   author = {Vinson Young and Chia-Chen Chou and Aamer Jaleel and Moinuddin Qureshi},
   title = {SHiP++: Enhancing Signature-Based Hit Predictor for Improved Cache Performance},
}

@article{llc,
   abstract = {Last-level caches (LLCs) are large structures with significant power requirements. They can be quite inefficient. On average, a cache block in a 2MB LRU-managed LLC is dead 86% of the time, i.e., it will not be referenced again before it is evicted. This paper introduces sampling dead block prediction, a technique that samples program counters (PCs) to determine when a cache block is likely to be dead. Rather than learning from accesses and evictions from every set in the cache, a sampling predictor keeps track of a small number of sets using partial tags. Sampling allows the predictor to use far less state than previous predictors to make predictions with superior accuracy. Dead block prediction can be used to drive a dead block replacement and bypass optimization. A sampling predictor can reduce the number of LLC misses over LRU by 11.7% for memory-intensive single-thread benchmarks and 23% for multi-core workloads. The reduction in misses yields a geometric mean speedup of 5.9% for single-thread benchmarks and a geometric mean normalized weighted speedup of 12.5% for multi-core workloads. Due to the reduced state and number of accesses, the sampling predictor consumes only 3.1% of the of the dynamic power and 1.2% of the leakage power of a baseline 2MB LLC, comparing favorably with more costly techniques. The sampling predictor can even be used to significantly improve a cache with a default random replacement policy.},
   author = {Samira Khan and Yingying Tian and Daniel A Jiménez},
   title = {Sampling Dead Block Prediction for Last-Level Caches},
}
@article{Elvira,
   abstract = {The disparity between last-level cache and memory latencies motivates the search for efficient cache management policies. Recent work in predicting reuse of cache blocks enables optimizations that significantly improve cache performance and efficiency. However, the accuracy of the prediction mechanisms limits the scope of optimization. This paper proposes perceptron learning for reuse prediction. The proposed predictor greatly improves accuracy over previous work. For multi-programmed workloads, the average false positive rate of the proposed predictor is 3.2%, while sampling dead block prediction (SDBP) and signature-based hit prediction (SHiP) yield false positive rates above 7%. The improvement in accuracy translates directly into performance. For single-thread workloads and a 4MB last-level cache, reuse prediction with perceptron learning enables a replacement and bypass optimization to achieve a geometric mean speedup of 6.1%, compared with 3.8% for SHiP and 3.5% for SDBP on the SPEC CPU 2006 benchmarks. On a memory-intensive subset of SPEC, perceptron learning yields 18.3% speedup, versus 10.5% for SHiP and 7.7% for SDBP. For multi-programmed workloads and a 16MB cache, the proposed technique doubles the efficiency of the cache over LRU and yields a geometric mean normalized weighted speedup of 7.4%, compared with 4.4% for SHiP and 4.2% for SDBP.},
   author = {Elvira Teran and Zhe Wang and Daniel A Jiménez},
   isbn = {9781509035083},
   title = {Perceptron Learning for Reuse Prediction},
}
@article{Jinchun,
   abstract = {Data prefetching and cache replacement algorithms have been intensively studied in the design of high performance microprocessors. Typically, the data prefetcher operates in the private caches and does not interact with the replacement policy in the shared Last-Level Cache (LLC). Similarly , most replacement policies do not consider demand and prefetch requests as different types of requests. In particular , program counter (PC)-based replacement policies cannot learn from prefetch requests since the data prefetcher does not generate a PC value. PC-based policies can also be negatively affected by compiler optimizations. In this paper, we propose a holistic cache management technique called Kill-the-PC (KPC) that overcomes the weaknesses of traditional prefetching and replacement policy algorithms. KPC cache management has three novel contributions. First, a prefetcher which approximates the future use distance of prefetch requests based on its prediction confidence. Second, a simple replacement policy provides similar or better performance than current state-of-the-art PC-based prediction using global hysteresis. Third, KPC integrates prefetching and replacement policy into a whole system which is greater than the sum of its parts. Information from the prefetcher is used to improve the performance of the replacement policy and vice-versa. Finally, KPC removes the need to propagate the PC through entire on-chip cache hierarchy while providing a holistic cache management approach with better performance than state-of-the-art PC-, and non-PC-based schemes. Our evaluation shows that KPC provides 8% better performance than the best combination of existing prefetcher and replacement policy for multi-core workloads.},
   author = {Jinchun Kim and Elvira Teran and Paul V Gratz and Daniel A Jiménez and Seth H Pugsley and Chris Wilkerson},
   doi = {10.1145/3037697.3037701},
   isbn = {9781450344654},
   keywords = {B32 [Memory Struc-tures]: Cache memories Keywords Memory Hierarchy,Cache Replacement Policy,Data Prefetching},
   publisher = {ACM},
   title = {Kill the Program Counter: Reconstructing Program Behavior in the Processor Cache Hierarchy},
   url = {http://dx.doi.org/10.1145/3037697.3037701},
}

@article{Wu2011,
   abstract = {The shared last-level caches in CMPs play an important role in improving application performance and reducing off-chip memory bandwidth requirements. In order to use LLCs more efficiently, recent research has shown that changing the re-reference prediction on cache insertions and cache hits can significantly improve cache performance. A fundamental challenge, however, is how to best predict the re-reference pattern of an incoming cache line. This paper shows that cache performance can be improved by correlating the re-reference behavior of a cache line with a unique signature. We investigate the use of memory region, program counter, and instruction sequence history based signatures. We also propose a novel Signature-based Hit Predictor (SHiP) to learn the re-reference behavior of cache lines belonging to each signature. Overall, we find that SHiP offers substantial improvements over the baseline LRU replacement and state-of-the-art replacement policy proposals. On average, SHiP improves sequential and multipro-grammed application performance by roughly 10% and 12% over LRU replacement, respectively. Compared to recent replacement policy proposals such as Seg-LRU and SDBP, SHiP nearly doubles the performance gains while requiring less hardware overhead.},
   author = {Carole-Jean Wu and Aamer Jaleel and Will Hasenplaugh and Margaret Martonosi and Simon C Steely and Joel Emer},
   isbn = {9781450310536},
   keywords = {B83 [Hardware]: Memory Structures General Terms Design,Performance Keywords Replacement,Reuse Distance Prediction,Shared Cache},
   title = {SHiP: Signature-based Hit Predictor for High Performance Caching},
   year = {2011},
}
@web_page{Aamer,
   title = {SHiP_Cache_Replacement_Policy/report.pdf at master · sachinpuranik99/SHiP_Cache_Replacement_Policy},
   url = {https://github.com/sachinpuranik99/SHiP_Cache_Replacement_Policy/blob/master/report.pdf},
}
@book{Aamer2010,
   abstract = {Title from The ACM Digital Library.},
   author = {André. Seznec and ACM Digital Library. and Sigarch.},
   isbn = {9781450300537},
   pages = {510},
   publisher = {ACM},
   title = {Proceedings of the 37th annual international symposium on Computer architecture.},
   year = {2010},
}
@report{Young2017,
   abstract = {In this paper, we analyze the state-of-the-art replacement policy based on Signature-Based Hit Prediction (SHiP). While we observe that the proposed SHiP implementation improves performance over the LRU policy on all configurations, we identify three areas of improvement. First, the policies for updating the counters in the Signature History Counter Table (SHCT) can be modified. Second, a broader range of insertion points for an incoming line can be used based on the value of the SHCT counter. Third, cache performance under prefetching can be enhanced by using a separate SHCT table for demand and prefetch lines and modifying the inser-tion/promotion of prefetch lines. We call our proposed implementation of SHiP with these enhancements as SHiP++. We evaluate SHiP++ on 17 benchmarks from the SPEC2006 suite. For the single-core without prefetching configuration, SHiP++ improves performance by 6.2% over LRU and 2.2% over SHiP. For the single-core with prefetching configuration, SHiP++ improves performance by 4.6% over LRU and 2.2% over SHiP. SHiP++ incurs an overhead of less than 20KB for managing the 2MB cache, well within the provisioned 32KB.},
   author = {Vinson Young and Chia-Chen Chou and Aamer Jaleel and Moinuddin Qureshi},
   title = {SHiP++: Enhancing Signature-Based Hit Predictor for Improved Cache Performance},
}
@article{Samira.1,
   abstract = {Last-level caches (LLCs) are large structures with significant power requirements. They can be quite inefficient. On average, a cache block in a 2MB LRU-managed LLC is dead 86% of the time, i.e., it will not be referenced again before it is evicted. This paper introduces sampling dead block prediction, a technique that samples program counters (PCs) to determine when a cache block is likely to be dead. Rather than learning from accesses and evictions from every set in the cache, a sampling predictor keeps track of a small number of sets using partial tags. Sampling allows the predictor to use far less state than previous predictors to make predictions with superior accuracy. Dead block prediction can be used to drive a dead block replacement and bypass optimization. A sampling predictor can reduce the number of LLC misses over LRU by 11.7% for memory-intensive single-thread benchmarks and 23% for multi-core workloads. The reduction in misses yields a geometric mean speedup of 5.9% for single-thread benchmarks and a geometric mean normalized weighted speedup of 12.5% for multi-core workloads. Due to the reduced state and number of accesses, the sampling predictor consumes only 3.1% of the of the dynamic power and 1.2% of the leakage power of a baseline 2MB LLC, comparing favorably with more costly techniques. The sampling predictor can even be used to significantly improve a cache with a default random replacement policy.},
   author = {Samira Khan and Yingying Tian and Daniel A Jiménez},
   title = {Sampling Dead Block Prediction for Last-Level Caches},
}

@article{kill,
   abstract = {Data prefetching and cache replacement algorithms have been intensively studied in the design of high performance microprocessors. Typically, the data prefetcher operates in the private caches and does not interact with the replacement policy in the shared Last-Level Cache (LLC). Similarly , most replacement policies do not consider demand and prefetch requests as different types of requests. In particular , program counter (PC)-based replacement policies cannot learn from prefetch requests since the data prefetcher does not generate a PC value. PC-based policies can also be negatively affected by compiler optimizations. In this paper, we propose a holistic cache management technique called Kill-the-PC (KPC) that overcomes the weaknesses of traditional prefetching and replacement policy algorithms. KPC cache management has three novel contributions. First, a prefetcher which approximates the future use distance of prefetch requests based on its prediction confidence. Second, a simple replacement policy provides similar or better performance than current state-of-the-art PC-based prediction using global hysteresis. Third, KPC integrates prefetching and replacement policy into a whole system which is greater than the sum of its parts. Information from the prefetcher is used to improve the performance of the replacement policy and vice-versa. Finally, KPC removes the need to propagate the PC through entire on-chip cache hierarchy while providing a holistic cache management approach with better performance than state-of-the-art PC-, and non-PC-based schemes. Our evaluation shows that KPC provides 8% better performance than the best combination of existing prefetcher and replacement policy for multi-core workloads.},
   author = {Jinchun Kim and Elvira Teran and Paul V Gratz and Daniel A Jiménez and Seth H Pugsley and Chris Wilkerson},
   doi = {10.1145/3037697.3037701},
   isbn = {9781450344654},
   keywords = {B32 [Memory Struc-tures]: Cache memories Keywords Memory Hierarchy,Cache Replacement Policy,Data Prefetching},
   publisher = {ACM},
   title = {Kill the Program Counter: Reconstructing Program Behavior in the Processor Cache Hierarchy},
   url = {http://dx.doi.org/10.1145/3037697.3037701},
}
@article{Seshadri2015,
   abstract = {Many modern high-performance processors prefetch blocks into the on-chip cache. Prefetched blocks can potentially pollute the cache by evicting more useful blocks. In this work, we observe that both accurate and inaccurate prefetches lead to cache pollution, and propose a comprehensive mechanism to mitigate prefetcher-caused cache pollution. First, we observe that over 95% of useful prefetches in a wide variety of applications are not reused after the first demand hit (in secondary caches). Based on this observation, our first mechanism simply demotes a prefetched block to the lowest priority on a demand hit. Second, to address pollution caused by inaccurate prefetches, we propose a self-tuning prefetch accuracy predictor to predict if a prefetch is accurate or inaccurate. Only predicted-accurate prefetches are inserted into the cache with a high priority. Evaluations show that our final mechanism, which combines these two ideas, significantly improves performance compared to both the baseline LRU policy and two state-of-the-art approaches to mitigating prefetcher-caused cache pollution (up to 49%, and 6% on average for 157 two-core multiprogrammed work-loads). The performance improvement is consistent across a wide variety of system configurations.. 2015. Mitigating prefetcher-caused pollution using informed caching policies for prefetched blocks.},
   author = {Vivek Seshadri and Samihan Yedkar and Hongyi Xin and Onur Mutlu and Phillip B Gibbons and Michael A Kozuch and Intel Pittsburgh and Todd C Mowry},
   doi = {10.1145/2677956},
   journal = {ACM Trans. Architec. Code Optim},
   keywords = {B32 [Memory Structures]: Design Styles General Terms: Design,Memory Additional Key Words and Phrases: Prefetching,Performance,cache insertion/promotion policy ACM Reference Format:,cache pollution,caches},
   title = {Mitigating Prefetcher-Caused Pollution Using Informed Caching Policies for Prefetched Blocks},
   volume = {11},
   url = {http://dx.doi.org/10.1145/2677956},
   year = {2015},
}
@article{Wu2011.1,
   abstract = {Hardware prefetching and last-level cache (LLC) management are two independent mechanisms to mitigate the growing latency to memory. However, the interaction between LLC management and hardware prefetching has received very little attention. This paper characterizes the performance of state-of-the-art LLC management policies in the presence and absence of hardware prefetching. Although prefetching improves performance by fetching useful data in advance, it can interact with LLC management policies to introduce application performance variability. This variability stems from the fact that current replacement policies treat prefetch and demand requests identically. In order to provide better and more predictable performance, we propose Prefetch-Aware Cache Management (PACMan). PACMan dynamically estimates and mitigates the degree of prefetch-induced cache interference by modifying the cache insertion and hit promotion policies to treat demand and prefetch requests differently. Across a variety of emerging workloads, we show that PACMan eliminates the performance variability in state-of-the-art replacement policies under the influence of prefetching. In fact, PAC-Man improves performance consistently across multimedia, games, server, and SPEC CPU2006 workloads by an average of 21.9% over the baseline LRU policy. For multiprogrammed workloads, on a 4-core CMP, PACMan improves performance by 21.5% on average.},
   author = {Carole-Jean Wu and Aamer Jaleel and Margaret Martonosi and Simon C Steely and Joel Emer},
   isbn = {9781450310536},
   keywords = {B83 [Hardware]: Memory Structures General Terms Design,Performance Keywords Prefetch-Aware Replacement,Reuse Distance Prediction,Set Dueling,Shared Cache},
   title = {PACMan: Prefetch-Aware Cache Management for High Performance Caching},
   year = {2011},
}
@article{San,
   author = {San Francisco and Sorav Bansal and Dharmendra S Modha},
   title = {USENIX Association Proceedings of the Third USENIX Conference on File and Storage Technologies CAR: Clock with Adaptive Replacement},
}
@article{arka,
   abstract = {Addresses suffering from cache misses typically exhibit repetitive patterns due to the temporal locality inherent in the access stream. However, we observe that the number of intervening misses at the last-level cache between the eviction of a particular block and its reuse can be very large, preventing traditional victim caching mechanisms from exploiting this repeating behavior. In this paper, we present Scavenger, a new architecture for last-level caches. Scavenger divides the total storage budget into a conventional cache and a novel victim file architecture, which employs a skewed Bloom filter in conjunction with a pipelined priority heap to identify and retain the blocks that most frequently missed in the conventional part of the cache in the recent past. When compared against a baseline configuration with a 1MB 8-way L2 cache, a Scavenger configuration with a 512kB 8-way conventional cache and a 512kB victim file achieves an IPC improvement of up to 63% and on average (geometric mean) 14.2% for nine memory-bound SPEC 2000 applications. On a larger set of sixteen SPEC 2000 applications, Scavenger achieves an average speedup of 8%.},
   author = {Arkaprava Basu and Nevin Kırman and Meyrem Kırman and Mainak Chaudhuri and José F Martínez},
   title = {Scavenger: A New Last Level Cache Architecture with Global Block Priority},
   url = {http://m3.csl.cornell.edu/},
}
@article{Belady,
   abstract = {This study i s based on a virtual-storage concept that provides for automatic memory allocation. Several algorithms for the replacement of current information in memory are evaluated. Discussed i s the simulation of a number of typical program runs using differing replacement algorithms with varying memory sixe and block size. The results are compared with each other and with a theoretical optimum. One of the basic limitations of a digital computer is the size of its available memory.' I n most cases, it is neither feasible nor economical for a user to insist that every problem program fit into memory. The number of words of information in a program often exceeds the number of cells (i.e., word locations) in memory. The only way to solve this problem is to assign more than one program word to a cell. Since a cell can hold only one word at a time, extra words assigned to the cell must be held in external storage. Conventionally, overlay techniques are employed to exchange memory words and external-storage words whenever needed; this, of course, places an additional planning and coding burden on the programmer. For several reasons, it would be advantageous to rid the programmer of this function by providing him with a "virtual" memory larger than his program. An approach that permits him to use a sufficiently large address range can accomplish this objective, assuming that means are provided for automatic execution of the memory-overlay functions. Among the first and most promising of the large-address approaches is the one described by Kilburn, et a1.' A similar virtual-addressing scheme was assumed as a starting point for the studies reported in this paper. Within this framework, the relative merits of various specific algorithms are compared. Before 78 IBM SYSTEMS},
   author = {L A Belady},
   title = {A study of replacement algorithms for a virtual-storage computer},
}
@article{Evan,
   author = {Evan Michael Purcell},
   journal = {Entrepreneurship The Journal of Business},
   pages = {5-15},
   title = {The Family LLC: A New Approach to Insuring Dynastic Wealth The Family LLC: A New Approach to Insuring Dynastic Wealth Recommended Citation Recommended Citation THE FAMILY LLC: A NEW APPROACH TO INSURING DYNASTIC WEALTH},
   volume = {8},
   url = {https://digitalcommons.pepperdine.edu/jbel/vol8/iss2/3},
}
@article{ADueling,
   abstract = {In this paper we present a high performance cache replacement algorithm called Dueling Segmented LRU replacement algorithm with adaptive Bypassing (DSB). The base algorithm is Segmented LRU (SLRU) replacement algorithm originally proposed for disk cache management. We introduce three enhancements to the base SLRU algorithm. First, a newly allocated line could be randomly promoted for better protection. Second, an aging algorithm is used to remove stale cache lines. Most importantly, we propose a novel scheme to track whether cache bypassing is effective. Based on the tracking results, we can adaptively adjust bypassing to fit workload behavior. DSB algorithm is implemented with a policy selector to dynamically select two variants of SLRU algorithms with different enhancements.},
   author = {Hongliang Gao and Chris Wilkerson},
   title = {A Dueling Segmented LRU Replacement Algorithm with Adaptive Bypassing},
}
@article{Mal2017,
   author = {Rano Mal},
   keywords = {Applied sciences},
   title = {A Simple Multi-Core Functional Cache Design Simulator},
   url = {https://scholarworks.utrgv.edu/etd/271},
   year = {2017},
}
@article{Samira,
   abstract = {Caches mitigate the long memory latency that limits the performance of modern processors. However, caches can be quite inefficient. On average, a cache block in a 2MB L2 cache is dead 59% of the time, i.e., it will not be referenced again before it is evicted. Increasing cache efficiency can improve performance by reducing miss rate, or alternately, improve power and energy by allowing a smaller cache with the same miss rate. This paper proposes using predicted dead blocks to hold blocks evicted from other sets. When these evicted blocks are referenced again, the access can be satisfied from the other set, avoiding a costly access to main memory. The pool of predicted dead blocks can be thought of as a virtual victim cache. For a set of memory-intensive single-threaded workloads, a virtual victim cache in a 16-way set associative 2MB L2 cache reduces misses by 26%, yields an geometric mean speedup of 12.1% and improves cache efficiency by 27% on average, where cache efficiency is defined as the average time during which cache blocks contain live information. This virtual victim cache yields a lower average miss rate than a fully-associative LRU cache of the same capacity. For a set of multi-core workloads, the virtual victim cache improves throughput performance by 4% over LRU while improving cache efficiency by 62%. Alternately, a 1.7MB virtual victim cache achieves about the same performance as a larger 2MB L2 cache, reducing the number of SRAM cells required by 16%, thus maintaining performance while reducing power and area.},
   author = {Samira Khan and Daniel A Jiménez and Doug Burger and Babak Falsafi},
   isbn = {9781450301787},
   issue = {10},
   keywords = {B32 [Hardware]: Memory Structures-Cache Memories General Terms Design,Performance Keywords microarchitecture,cache management,prediction},
   title = {Using Dead Blocks as a Virtual Victim Cache},
}
@web_page{,
   title = {IEEE Xplore Full-Text PDF:},
   url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4358260},
}
@article{Dong,
   abstract = {We show that there exists a spectrum of block replacement policies that subsumes both the Least Recently Used (LRU) and the Least Frequently Used (LFU) policies. The spectrum is formed according to how much more weight we give to the recent history than to the older history, and is referred to as the LRFU (Least Recently/Frequently Used) policy. Unlike many previous policies that use limited history to make block replacement decisions, the LRFU policy uses the complete reference history of blocks recorded during their cache residency. Nevertheless, the LRFU requires only a few words for each block to maintain such history. This paper also describes an implementation of the LRFU that again subsumes the LRU and LFU implementations. The LRFU policy is applied to buffer caching, and results from trace-driven simulations show that the LRFU performs better than previously known policies for the workloads we considered. This point is reinforced by results from our integration of the LRFU into the FreeBSD operating system.},
   author = {Donghee Leet and Jongmoo Choit and Jong-Hun Kim+ and Abstract Sam and H Nohs},
   title = {On the Existence of a Spectrum of Policies that Subsumes the Least Recently Used (LRU) and Least Frequently Used (LFU) Policies},
   url = {http://ssrnet.snu.ac.krhttp://archi.snu.ac.kr},
}
@article{Haiming,
   abstract = {Data caches in general-purpose microprocessors often contain mostly dead blocks and are thus used inefficiently. To improve cache efficiency, dead blocks should be identified and evicted early. Prior schemes predict the death of a block immediately after it is accessed; however, these schemes yield lower prediction accuracy and coverage. Instead, we find that predicting the death of a block when it just moves out of the MRU position gives the best tradeoff between timeliness and prediction accuracy/coverage. Furthermore, the individual reference history of a block in the L1 cache can be irregular because of data/control dependence. This paper proposes a new class of dead-block predictors that predict dead blocks based on bursts of accesses to a cache block. A cache burst begins when a block becomes MRU and ends when it becomes non-MRU. Cache bursts are more predictable than individual references because they hide the irregularity of individual references. When used at the L1 cache, the best burst-based predictor can identify 96% of the dead blocks with a 96% accuracy. With the improved dead-block predictors, we evaluate three ways to increase cache efficiency by eliminating dead blocks early: replacement optimization, bypassing, and prefetching. The most effective approach, prefetching into dead blocks, increases the average L1 efficiency from 8% to 17% and the L2 efficiency from 17% to 27%. This increased cache efficiency translates into higher overall performance: prefetching into dead blocks outperforms the same prefetch scheme without dead-block prediction by 12% at the L1 and by 13% at the L2.},
   author = {Haiming Liu and Michael Ferdman and Jaehyuk Huh and Doug Burger},
   title = {Cache Bursts: A New Approach for Eliminating Dead Blocks and Increasing Cache Efficiency},
}
@article{Robust,
   abstract = {Robust and powerful software instrumentation tools are essential for program analysis tasks such as profiling, performance evaluation , and bug detection. To meet this need, we have developed a new instrumentation system called Pin. Our goals are to provide easy-to-use, portable, transparent, and efficient instrumenta-tion. Instrumentation tools (called Pintools) are written in C/C++ using Pin's rich API. Pin follows the model of ATOM, allowing the tool writer to analyze an application at the instruction level without the need for detailed knowledge of the underlying instruction set. The API is designed to be architecture independent whenever possible, making Pintools source compatible across different archi-tectures. However, a Pintool can access architecture-specific details when necessary. Instrumentation with Pin is mostly transparent as the application and Pintool observe the application's original, unin-strumented behavior. Pin uses dynamic compilation to instrument executables while they are running. For efficiency, Pin uses several techniques, including inlining, register reallocation , liveness analysis, and instruction scheduling to optimize instrumentation. This fully automated approach delivers significantly better instru-mentation performance than similar tools. For example, Pin is 3.3x faster than Valgrind and 2x faster than DynamoRIO for basic-block counting. To illustrate Pin's versatility, we describe two Pintools in daily use to analyze production software. Pin is publicly available for Linux platforms on four architectures: IA32 (32-bit x86), EM64T (64-bit x86), Itanium R V , and ARM. In the ten months since Pin 2 was released in July 2004, there have been over 3000 down-loads from its website.},
   author = {Chi-Keung Luk and Robert Cohn and Robert Muth and Harish Patil and Artur Klauser and Geoff Lowney and Steven Wallace and Vijay Janapa Reddi and Kim Hazelwood},
   keywords = {D25 [Software Engineer-ing]: Testing and Debugging-code inspections and walk-throughs,Experimentation Keywords Instrumentation,Performance,debugging aids,dynamic com-pilation,incremental compilers General Terms Languages,program analysis tools,tracing; D34 [Programming Languages]: Processors-compilers},
   title = {Pin: Building Customized Program Analysis Tools with Dynamic Instrumentation},
}
@web_page{,
   title = {IEEE Xplore Full-Text PDF:},
   url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5749733},
}
@article{wecon,
   abstract = {We consider the problem of cache management in a demand paging scenario with uniform page sizes. We propose a new cache management policy, namely, Adaptive Replacement Cache (ARC), that has several advantages. In response to evolving and changing access patterns, ARC dynamically, adaptively, and continually balances between the recency and frequency components in an online and self-tuning fashion. The policy ARC uses a learning rule to adaptively and continually revise its assumptions about the workload. The policy ARC is empirically universal, that is, it empirically performs as well as a certain fixed replacement policy-even when the latter uses the best workload-specific tuning parameter that was selected in an offline fashion. Consequently , ARC works uniformly well across varied workloads and cache sizes without any need for workload specific a priori knowledge or tuning. Various policies such as LRU-2, 2Q, LRFU, and LIRS require user-defined parameters, and, unfortunately, no single choice works uniformly well across different workloads and cache sizes. The policy ARC is simple-to-implement and, like LRU, has constant complexity per request. In comparison, policies LRU-2 and LRFU both require logarithmic time complexity in the cache size. The policy ARC is scan-resistant: it allows one-time sequential requests to pass through without polluting the cache. On ¢ £ real-life traces drawn from numerous domains, ARC leads to substantial performance gains over LRU for a wide range of cache sizes. For example, for a SPC1 like synthetic benchmark, at 4GB cache, LRU delivers a hit ratio of ¥ ¦ ¨ ¥ while ARC achieves a hit ratio of ¢ .},
   author = {San Francisco and Nimrod Megiddo and Dharmendra S Modha},
   title = {USENIX Association Proceedings of FAST '03: 2nd USENIX Conference on File and Storage Technologies 2nd USENIX Conference on File and Storage Technologies USENIX Association 115 ARC: A SELF-TUNING, LOW OVERHEAD REPLACEMENT CACHE},
}
@article{Xie2009,
   abstract = {Many multi-core processors employ a large last-level cache (LLC) shared among the multiple cores. Past research has demonstrated that sharing-oblivious cache management policies (e.g., LRU) can lead to poor performance and fairness when the multiple cores compete for the limited LLC capacity. Different memory access patterns can cause cache contention in different ways, and various techniques have been proposed to target some of these behaviors. In this work, we propose a new cache management approach that combines dynamic insertion and promotion policies to provide the benefits of cache partitioning, adaptive insertion, and capacity stealing all with a single mechanism. By handling multiple types of memory behaviors, our proposed technique outperforms techniques that target only either capacity partitioning or adaptive insertion.},
   author = {Yuejian Xie and Gabriel H Loh},
   isbn = {9781605585260},
   keywords = {C12 [Processor Architectures]: Multiple Data Stream Architec-tures General Terms Design,Performance Keywords Multi-core,cache,contention,insertion,promotion,sharing},
   title = {PIPP: Promotion/Insertion Pseudo-Partitioning of Multi-Core Shared Caches},
   year = {2009},
}
@article{Santosh,
   abstract = {High performance processors employ hardware data prefetching to reduce the negative performance impact of large main memory latencies. While prefetching improves performance substantially on many programs, it can significantly reduce performance on others. Also, prefetching can significantly increase memory bandwidth requirements. This paper proposes a mechanism that incorporates dynamic feedback into the design of the prefetcher to increase the performance improvement provided by prefetching as well as to reduce the negative performance and bandwidth impact of prefetching. Our mechanism estimates prefetcher accuracy, prefetcher timeliness, and prefetcher-caused cache pollution to adjust the aggressiveness of the data prefetcher dynamically. We introduce a new method to track cache pollution caused by the prefetcher at run-time. We also introduce a mechanism that dynamically decides where in the LRU stack to insert the prefetched blocks in the cache based on the cache pollution caused by the prefetcher. Using the proposed dynamic mechanism improves average performance by 6.5% on 17 memory-intensive benchmarks in the SPEC CPU2000 suite compared to the best-performing conventional stream-based data prefetcher configuration, while it consumes 18.7% less memory bandwidth. Compared to a conventional stream-based data prefetcher configuration that consumes similar amount of memory bandwidth, feedback directed prefetching provides 13.6% higher performance. Our results show that feedback-directed prefetching eliminates the large negative performance impact incurred on some benchmarks due to prefetching, and it is applicable to stream-based prefetchers, global-history-buffer based delta correlation prefetchers, and PC-based stride prefetchers.},
   author = {Santhosh Srinath and Onur Mutlu and Hyesoon Kim and Yale N Patt},
   title = {Feedback Directed Prefetching: Improving the Performance and Bandwidth-Efficiency of Hardware Prefetchers},
}
@web_page{,
   title = {IEEE Xplore Full-Text PDF:},
   url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5375363},
}
@article{,
   title = {THE ADVANCED COMPUTING SYSTEMS ASSOCIATION},
   year = {2001},
}
@article{Song,
   abstract = {Addresses suffering from cache misses typically exhibit repetitive patterns due to the temporal locality inherent in the access stream. However, we observe that the number of intervening misses at the last-level cache between the eviction of a particular block and its reuse can be very large, preventing traditional victim caching mechanisms from exploiting this repeating behavior. In this paper, we present Scavenger, a new architecture for last-level caches. Scavenger divides the total storage budget into a conventional cache and a novel victim file architecture, which employs a skewed Bloom filter in conjunction with a pipelined priority heap to identify and retain the blocks that most frequently missed in the conventional part of the cache in the recent past. When compared against a baseline configuration with a 1MB 8-way L2 cache, a Scavenger configuration with a 512kB 8-way conventional cache and a 512kB victim file achieves an IPC improvement of up to 63% and on average (geometric mean) 14.2% for nine memory-bound SPEC 2000 applications. On a larger set of sixteen SPEC 2000 applications, Scavenger achieves an average speedup of 8%.},
   author = {Arkaprava Basu and Nevin Kırman and Meyrem Kırman and Mainak Chaudhuri and José F Martínez},
   title = {Scavenger: A New Last Level Cache Architecture with Global Block Priority},
   url = {http://m3.csl.cornell.edu/},
}
@article{Jiang,
   author = {Song Jiang and Xiaodong Zhang},
   title = {LIRS: An Efficient Low Inter-reference Recency Set Replacement Policy to Improve Buffer Cache Performance},
}
@book{Seznec2010.1,
   abstract = {Title from The ACM Digital Library.},
   author = {André. Seznec and ACM Digital Library. and Sigarch.},
   isbn = {9781450300537},
   pages = {510},
   publisher = {ACM},
   title = {Proceedings of the 37th annual international symposium on Computer architecture.},
   year = {2010},
}
@web_page{,
   title = {IEEE Xplore Full-Text PDF:},
   url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4358260},
}
@article{Kharbutli2008.1,
   abstract = {Recent studies have shown that, in highly associative caches, the performance gap between the Least Recently Used (LRU) and the theoretical optimal replacement algorithms is large, motivating the design of alternative replacement algorithms to improve cache performance. In LRU replacement, a line, after its last use, remains in the cache for a long time until it becomes the LRU line. Such dead lines unnecessarily reduce the cache capacity available for other lines. In addition, in multilevel caches, temporal reuse patterns are often inverted, showing in the L1 cache but, due to the filtering effect of the L1 cache, not showing in the L2 cache. At the L2, these lines appear to be brought in the cache but are never reaccessed until they are replaced. These lines unnecessarily pollute the L2 cache. This paper proposes a new counter-based approach to deal with the above problems. For the former problem, we predict lines that have become dead and replace them early from the L2 cache. For the latter problem, we identify never-reaccessed lines, bypass the L2 cache, and place them directly in the L1 cache. Both techniques are achieved through a single counter-based mechanism. In our approach, each line in the L2 cache is augmented with an event counter that is incremented when an event of interest such as certain cache accesses occurs. When the counter reaches a threshold, the line 'expires' and becomes replaceable. Each line's threshold is unique and is dynamically learned. We propose and evaluate two new replacement algorithms: Access Interval Predictor (AIP) and Live-time Predictor (LvP). AIP and LvP speed up 10 capacity-constrained SPEC2000 benchmarks by up to 48 percent and 15 percent on average (7 percent on average for the whole 21 Spec2000 benchmarks). Cache bypassing further reduces L2 cache pollution and improves the average speedups to 17 percent (8 percent for the whole 21 Spec2000 benchmarks). © 2008 IEEE.},
   author = {Mazen Kharbutli and Yan Solihin},
   doi = {10.1109/TC.2007.70816},
   issn = {00189340},
   issue = {4},
   journal = {IEEE Transactions on Computers},
   keywords = {Cache bypassing,Cache misses,Cache replacement algorithms,Caches,Counter-based algorithms},
   month = {4},
   pages = {433-447},
   title = {Counter-based cache replacement and bypassing algorithms},
   volume = {57},
   year = {2008},
}
@article{Liu,
   abstract = {Data caches in general-purpose microprocessors often contain mostly dead blocks and are thus used inefficiently. To improve cache efficiency, dead blocks should be identified and evicted early. Prior schemes predict the death of a block immediately after it is accessed; however, these schemes yield lower prediction accuracy and coverage. Instead, we find that predicting the death of a block when it just moves out of the MRU position gives the best tradeoff between timeliness and prediction accuracy/coverage. Furthermore, the individual reference history of a block in the L1 cache can be irregular because of data/control dependence. This paper proposes a new class of dead-block predictors that predict dead blocks based on bursts of accesses to a cache block. A cache burst begins when a block becomes MRU and ends when it becomes non-MRU. Cache bursts are more predictable than individual references because they hide the irregularity of individual references. When used at the L1 cache, the best burst-based predictor can identify 96% of the dead blocks with a 96% accuracy. With the improved dead-block predictors, we evaluate three ways to increase cache efficiency by eliminating dead blocks early: replacement optimization, bypassing, and prefetching. The most effective approach, prefetching into dead blocks, increases the average L1 efficiency from 8% to 17% and the L2 efficiency from 17% to 27%. This increased cache efficiency translates into higher overall performance: prefetching into dead blocks outperforms the same prefetch scheme without dead-block prediction by 12% at the L1 and by 13% at the L2.},
   author = {Haiming Liu and Michael Ferdman and Jaehyuk Huh and Doug Burger},
   title = {Cache Bursts: A New Approach for Eliminating Dead Blocks and Increasing Cache Efficiency},
}
@article{Qureshi2007.1,
   abstract = {The commonly used LRU replacement policy is susceptible to thrashing for memory-intensive workloads that have a working set greater than the available cache size. For such applications, the majority of lines traverse from the MRU position to the LRU position without receiving any cache hits, resulting in inefficient use of cache space. Cache performance can be improved if some fraction of the working set is retained in the cache so that at least that fraction of the working set can contribute to cache hits. We show that simple changes to the insertion policy can significantly reduce cache misses for memory-intensive workloads. We propose the LRU Insertion Policy (LIP) which places the incoming line in the LRU position instead of the MRU position. LIP protects the cache from thrashing and results in close to optimal hit-rate for applications that have a cyclic reference pattern. We also propose the Bimodal Insertion Policy (BIP) as an enhancement of LIP that adapts to changes in the working set while maintaining the thrashing protection of LIP. We finally propose a Dynamic Insertion Policy (DIP) to choose between BIP and the traditional LRU policy depending on which policy incurs fewer misses. The proposed insertion policies do not require any change to the existing cache structure, are trivial to implement, and have a storage requirement of less than two bytes. We show that DIP reduces the average MPKI of the baseline 1MB 16-way L2 cache by 21%, bridging two-thirds of the gap between LRU and OPT.},
   author = {Moinuddin K Qureshi and Aamer Jaleel and Yale N Patt and Simon C Steely Jr and Joel Emer},
   isbn = {9781595937063},
   keywords = {B32 [Design Styles]: Cache memories General Terms: Design,Performance Keywords: Replacement,Set Dueling,Set Sampling,Thrashing},
   title = {Adaptive Insertion Policies for High Performance Caching},
   year = {2007},
}
@article{Onur,
   abstract = {High performance processors employ hardware data prefetching to reduce the negative performance impact of large main memory latencies. While prefetching improves performance substantially on many programs, it can significantly reduce performance on others. Also, prefetching can significantly increase memory bandwidth requirements. This paper proposes a mechanism that incorporates dynamic feedback into the design of the prefetcher to increase the performance improvement provided by prefetching as well as to reduce the negative performance and bandwidth impact of prefetching. Our mechanism estimates prefetcher accuracy, prefetcher timeliness, and prefetcher-caused cache pollution to adjust the aggressiveness of the data prefetcher dynamically. We introduce a new method to track cache pollution caused by the prefetcher at run-time. We also introduce a mechanism that dynamically decides where in the LRU stack to insert the prefetched blocks in the cache based on the cache pollution caused by the prefetcher. Using the proposed dynamic mechanism improves average performance by 6.5% on 17 memory-intensive benchmarks in the SPEC CPU2000 suite compared to the best-performing conventional stream-based data prefetcher configuration, while it consumes 18.7% less memory bandwidth. Compared to a conventional stream-based data prefetcher configuration that consumes similar amount of memory bandwidth, feedback directed prefetching provides 13.6% higher performance. Our results show that feedback-directed prefetching eliminates the large negative performance impact incurred on some benchmarks due to prefetching, and it is applicable to stream-based prefetchers, global-history-buffer based delta correlation prefetchers, and PC-based stride prefetchers.},
   author = {Santhosh Srinath and Onur Mutlu and Hyesoon Kim and Yale N Patt},
   title = {Feedback Directed Prefetching: Improving the Performance and Bandwidth-Efficiency of Hardware Prefetchers},
}
@article{doi,
   abstract = {In chip multiprocessors (CMPs), several high-performance cores typically compete for capacity in a shared last-level cache. This causes degraded and unpredictable memory performance for multiprogrammed and parallel workloads. In response, recent schemes apportion cache bandwidth and capacity in ways that offer better aggregate performance for the workloads. These schemes, however, focus primarily on relatively coarse-grained capacity management without concern for operating system process priority levels. In this work, we explore capacity management approaches that are both temporally and spatially more fine-grained than prior work. We also consider operating system priority levels as part of capacity management. We propose a capacity management mechanism based on timekeeping techniques that track the time interval since the last access to cached data. This Adaptive Timekeeping Replacement (ATR) scheme maintains aggregate cache occupancies that reflect the priority and footprint of each application. The key novelties of our work are (1) ATR offers a complete cache capacity management framework taking into account application priorities and memory characteristics, and (2) ATR's fine-grained cache capacity control is demonstrated to be effective and important in improving the performance of parallel workloads in addition to sequential ones. We evaluate our ideas using a full-system simulator and multiprogrammed workloads of both sequential and parallel applications. This is the first detailed study of shared cache capacity management considering thread behaviors in parallel applications. ATR outperforms an unmanaged system by as much as 1.63X and by an average of 1.19X. ATR's fine-grained temporal control is particularly important for parallel applications , which are expected to be increasingly prevalent in years to come.},
   author = {C Acm Reference Format: Wu and M Martonosi},
   doi = {10.1145/0000000.0000000},
   issue = {11},
   journal = {ACM Trans. Architec. Code Optim},
   keywords = {B83 [Hardware]: Memory Structures-General General Terms: Design,Performance Additional Key Words and Phrases: Cache decay,capacity management,shared resource management},
   pages = {27},
   title = {Adaptive Timekeeping Replacement: Fine-Grained Capacity Management for Shared CMP Caches},
   volume = {11},
   url = {http://doi.acm.org/10.1145/0000000.0000000},
   year = {2010},
}
